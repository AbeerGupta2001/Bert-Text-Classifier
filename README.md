# ONNX-Optimized Text Classification App (Streamlit + Hugging Face)

## Overview

This project is a **high-performance text classification web application** built using **Streamlit** and **Hugging Face Transformers**, with inference optimized using **ONNX Runtime**.  
The application demonstrates how a fine-tuned Transformer model can be converted to ONNX for **faster, CPU-efficient inference** and deployed in a **containerized environment using Docker**.

The project is suitable for:

- Real-time NLP inference
- CPU-only deployment environments
- Resume-ready ML engineering demonstrations

---

## Key Features

- Transformer-based text classification
- ONNX Runtime for fast CPU inference (~2–4× speedup)
- Streamlit-based interactive UI
- Dockerized deployment (no virtual environment required)
- Optional Tor SOCKS proxy support for network routing experiments

---

## Tech Stack

- Python 3.10
- Streamlit
- Hugging Face Transformers
- ONNX Runtime
- Docker

## App link:

https://bert-text-classifier-8eiafe6hmkcwzgskdxs22g.streamlit.app/app
